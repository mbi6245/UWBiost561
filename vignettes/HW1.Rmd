---
title: "HW1"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{HW1}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Q0: Survey

1. Preferred name: Max
    1. Last name is "Bi", pronounced like "bee"
1. I have read and understood the entire syllabus
1. I've taken various CSE courses ranging from data structures/algorithms to machine learning to web development that utilized a variety of languages (Java, Python, Javascript, SQL, etc...) and of course numerous STAT courses that have required programming in R. As a result, I'm reasonably comfortable programming in R.
1. I'm interested in getting a personal website and experience using computing clusters/servers out of this course.

# Q1: 

```{r setup}
plot(1:10)
```

# Q2:

```{r}
source("https://raw.githubusercontent.com/linnykos/561_s2024_public/main/homework1/generate_data.R")
```

The function first checks whether or not n is a natural number, and then generates 2 vectors of length n. One of the vectors, `idx_vec`, is randomly filled with the numbers 1, 2, and 3 while the other vector, `sample_vec`, is empty. Then, at each position in `idx_vec`, the corresponding position in `sample_vec` will be filled with a draw from Normal(mean = 10, sd = 1)  if the former has a 1, a draw from Gamma(shape = 2, scale = 2) if the former has a 2, and a draw from Chisq(df = 3) if the former has a 3.

```{r}
reps = 10000
ns = c(1, 2, 3, 5, 10, 100)
mean(generate_data(n))
par(mfrow = c(2, 3))
res_mat = matrix(nrow = length(ns), ncol = reps)
for (i in 1:length(ns)) {
  for (j in 1:reps) {
    res_mat[i, j] = mean(generate_data(ns[i]))
  }
  hist(res_mat[i, ],
       breaks = 100,
       main = paste("Sample size: ", ns[i]),
       xlab = "Value")
}
```

The historgrams generated demonstrate the central limit theorem by confirming that as the sample size increases, repeated draws from the data create a normal distribution centered around the mean of the sampling distribution.

# Q3: Basic Data Analysis

```{r}
df <- read.csv("https://raw.githubusercontent.com/linnykos/561_s2024_public/main/homework1/sea-ad.csv")
```

The `head` function simply prints out the values of each of the column for the first 6 rows in the dataframe. The `summary` function prints out summary statistics for each of the columns if it is a numerical column, or the class of the data inside for non-numerical columns. NAs are also counted for numerical columns when running this function.

The `df` object is of class `r class(df)` and it's dimensionality is `r dim(df)`.

```{r}
df$Age.at.Death[df$Age.at.Death == "90+"] = 90
df$Age.at.Death = as.numeric(df$Age.at.Death)
hist(df$Age.at.Death)
```

```{r}
cols_to_convert = c("Sex", "APOE4.Status", "Cognitive.Status", "Braak")
df[cols_to_convert] = lapply(df[cols_to_convert], factor)
summary(df)
```

This is much more informative than the previous output because for the columns coded as variables of class factor, the reader is able to see the counts of each of the unique values in each column

```{r}
with(df, table(Braak, Cognitive.Status))
with(df, table(cut(Last.CASI.Score, quantile(Last.CASI.Score, na.rm = TRUE)), Cognitive.Status))
```

We can learn from the second `table` function output that those with a Last.CASI.Score in the lower 50th quantile typically have dementia and those with a Last.CASI.Score above the upper 50th quantile typicaldy do not have dementia.
